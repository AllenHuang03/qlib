#!/usr/bin/env python3
"""
Continuous Integration Testing Framework
AGENT 5: CONTINUOUS INTEGRATION SPECIALIST

Automated testing framework that runs daily test suites across all components
and user journeys implemented by the previous agents.
"""

import asyncio
import logging
import datetime
import json
import os
import sys
import subprocess
import psutil
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import requests
import pytest
import unittest

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

@dataclass
class TestResult:
    """Test result data structure"""
    test_name: str
    status: str  # 'passed', 'failed', 'skipped'
    duration: float
    error_message: Optional[str] = None
    details: Optional[Dict[str, Any]] = None

@dataclass
class TestSuiteResult:
    """Test suite result aggregation"""
    suite_name: str
    total_tests: int
    passed: int
    failed: int
    skipped: int
    duration: float
    tests: List[TestResult]
    coverage: Optional[float] = None

class ContinuousTestingFramework:
    """
    Comprehensive automated testing framework for the Qlib trading platform.
    
    Executes daily test suites across:
    - Customer journeys (Agents 1-2)
    - Staff workflows (Agent 3) 
    - Data insertion points (Agent 4)
    - System integration
    - Performance benchmarks
    """
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.logger = self._setup_logging()
        self.test_results: Dict[str, TestSuiteResult] = {}
        self.start_time = datetime.datetime.now()
        
        # Test environment URLs
        self.backend_url = self.config.get('backend_url', 'http://localhost:8000')
        self.frontend_url = self.config.get('frontend_url', 'http://localhost:5173')
        
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """Load testing configuration"""
        default_config = {
            'backend_url': 'http://localhost:8000',
            'frontend_url': 'http://localhost:5173',
            'test_timeout': 300,  # 5 minutes per test suite
            'parallel_tests': True,
            'coverage_threshold': 80.0,
            'performance_baseline': {
                'api_response_time': 2.0,  # seconds
                'page_load_time': 3.0,     # seconds
                'memory_usage': 512,       # MB
            },
            'notification': {
                'email_enabled': False,
                'slack_webhook': None,
                'teams_webhook': None
            }
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                user_config = json.load(f)
                default_config.update(user_config)
                
        return default_config
    
    def _setup_logging(self) -> logging.Logger:
        """Setup comprehensive logging"""
        logger = logging.getLogger('ContinuousTestingFramework')
        logger.setLevel(logging.INFO)
        
        # Create logs directory
        logs_dir = project_root / 'logs' / 'testing'
        logs_dir.mkdir(parents=True, exist_ok=True)
        
        # File handler with timestamp
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = logs_dir / f'continuous_testing_{timestamp}.log'
        
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    async def run_daily_test_suite(self) -> Dict[str, TestSuiteResult]:
        """
        Execute comprehensive daily test suite covering all platform components
        """
        self.logger.info("ðŸš€ Starting Daily Automated Test Suite")
        self.logger.info(f"Test environment: Backend={self.backend_url}, Frontend={self.frontend_url}")
        
        try:
            # Pre-flight checks
            await self._pre_flight_checks()
            
            # Execute test suites in parallel where possible
            tasks = []
            
            # 1. Customer Journey Tests (Agents 1-2)
            tasks.append(self._run_customer_journey_tests())
            
            # 2. Staff Workflow Tests (Agent 3)
            tasks.append(self._run_staff_workflow_tests())
            
            # 3. Data Integration Tests (Agent 4)
            tasks.append(self._run_data_integration_tests())
            
            # 4. System Integration Tests
            tasks.append(self._run_system_integration_tests())
            
            # 5. Performance Tests
            tasks.append(self._run_performance_tests())
            
            # 6. Security Tests
            tasks.append(self._run_security_tests())
            
            # Execute tests
            if self.config.get('parallel_tests', True):
                results = await asyncio.gather(*tasks, return_exceptions=True)
            else:
                results = []
                for task in tasks:
                    result = await task
                    results.append(result)
            
            # Process results
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"Test suite {i} failed with exception: {result}")
                    continue
                    
                if isinstance(result, TestSuiteResult):
                    self.test_results[result.suite_name] = result
            
            # Generate comprehensive report
            report = await self._generate_comprehensive_report()
            
            # Send notifications
            await self._send_notifications(report)
            
            return self.test_results
            
        except Exception as e:
            self.logger.error(f"Critical error in daily test suite: {e}")
            raise
        finally:
            self.logger.info(f"Daily test suite completed in {datetime.datetime.now() - self.start_time}")
    
    async def _pre_flight_checks(self) -> None:
        """Verify test environment is ready"""
        self.logger.info("ðŸ” Running pre-flight checks...")
        
        # Check backend health
        try:
            response = requests.get(f"{self.backend_url}/health", timeout=10)
            if response.status_code != 200:
                raise Exception(f"Backend health check failed: {response.status_code}")
            self.logger.info("âœ… Backend health check passed")
        except Exception as e:
            self.logger.error(f"âŒ Backend health check failed: {e}")
            raise
        
        # Check database connectivity
        try:
            response = requests.get(f"{self.backend_url}/api/test/db-health", timeout=10)
            if response.status_code == 200:
                self.logger.info("âœ… Database connectivity verified")
            else:
                self.logger.warning("âš ï¸ Database connectivity check inconclusive")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Database connectivity check failed: {e}")
        
        # Check test data availability
        self._verify_test_data()
        
        self.logger.info("âœ… Pre-flight checks completed")
    
    def _verify_test_data(self) -> None:
        """Verify test data files are available"""
        test_data_dir = project_root / 'backend' / 'test_data'
        required_files = [
            'test_portfolio_small.csv',
            'test_portfolio_10k_holdings.csv',
            'test_portfolio_edge_cases.csv',
            'test_scenarios_summary.json'
        ]
        
        for filename in required_files:
            file_path = test_data_dir / filename
            if not file_path.exists():
                self.logger.warning(f"âš ï¸ Test data file missing: {filename}")
            else:
                self.logger.info(f"âœ… Test data verified: {filename}")
    
    async def _run_customer_journey_tests(self) -> TestSuiteResult:
        """Test all customer journeys implemented by Agents 1-2"""
        self.logger.info("ðŸ›£ï¸ Running Customer Journey Tests...")
        
        start_time = time.time()
        tests = []
        
        # Test scenarios for each customer type
        customer_types = [
            'new_customer_onboarding',
            'verified_customer_flow', 
            'premium_customer_flow',
            'institutional_client_flow'
        ]
        
        for customer_type in customer_types:
            try:
                result = await self._test_customer_journey(customer_type)
                tests.append(result)
            except Exception as e:
                tests.append(TestResult(
                    test_name=f"customer_journey_{customer_type}",
                    status='failed',
                    duration=0,
                    error_message=str(e)
                ))
        
        duration = time.time() - start_time
        passed = len([t for t in tests if t.status == 'passed'])
        failed = len([t for t in tests if t.status == 'failed'])
        skipped = len([t for t in tests if t.status == 'skipped'])
        
        return TestSuiteResult(
            suite_name='customer_journeys',
            total_tests=len(tests),
            passed=passed,
            failed=failed,
            skipped=skipped,
            duration=duration,
            tests=tests
        )
    
    async def _test_customer_journey(self, customer_type: str) -> TestResult:
        """Test specific customer journey flow"""
        start_time = time.time()
        
        try:
            # Simulate customer journey based on type
            if customer_type == 'new_customer_onboarding':
                await self._test_new_customer_flow()
            elif customer_type == 'verified_customer_flow':
                await self._test_verified_customer_flow()
            elif customer_type == 'premium_customer_flow':
                await self._test_premium_customer_flow()
            elif customer_type == 'institutional_client_flow':
                await self._test_institutional_flow()
            
            duration = time.time() - start_time
            return TestResult(
                test_name=f"customer_journey_{customer_type}",
                status='passed',
                duration=duration,
                details={'customer_type': customer_type}
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return TestResult(
                test_name=f"customer_journey_{customer_type}",
                status='failed',
                duration=duration,
                error_message=str(e)
            )
    
    async def _test_new_customer_flow(self) -> None:
        """Test new customer onboarding process"""
        # Test registration
        registration_data = {
            'email': 'test@example.com',
            'password': 'SecurePassword123!',
            'firstName': 'Test',
            'lastName': 'User'
        }
        
        response = requests.post(
            f"{self.backend_url}/api/auth/register",
            json=registration_data,
            timeout=10
        )
        
        if response.status_code not in [200, 201]:
            raise Exception(f"Registration failed: {response.status_code}")
        
        # Test login
        login_data = {
            'email': registration_data['email'],
            'password': registration_data['password']
        }
        
        response = requests.post(
            f"{self.backend_url}/api/auth/login",
            json=login_data,
            timeout=10
        )
        
        if response.status_code != 200:
            raise Exception(f"Login failed: {response.status_code}")
        
        # Test plan selection
        user_token = response.json().get('token')
        headers = {'Authorization': f'Bearer {user_token}'}
        
        # Test accessing dashboard
        response = requests.get(
            f"{self.backend_url}/api/user/dashboard",
            headers=headers,
            timeout=10
        )
        
        if response.status_code != 200:
            raise Exception(f"Dashboard access failed: {response.status_code}")
    
    async def _test_verified_customer_flow(self) -> None:
        """Test verified customer features"""
        # Use test account
        response = requests.post(
            f"{self.backend_url}/api/test-accounts/create",
            json={'accountType': 'verified'},
            timeout=10
        )
        
        if response.status_code not in [200, 201]:
            raise Exception(f"Test account creation failed: {response.status_code}")
        
        account_data = response.json()
        headers = {'Authorization': f'Bearer {account_data.get("token")}'}
        
        # Test portfolio upload
        test_portfolio_path = project_root / 'backend' / 'test_data' / 'test_portfolio_small.csv'
        
        with open(test_portfolio_path, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.backend_url}/api/portfolio/upload",
                files=files,
                headers=headers,
                timeout=30
            )
        
        if response.status_code not in [200, 201]:
            raise Exception(f"Portfolio upload failed: {response.status_code}")
        
        # Test model access
        response = requests.get(
            f"{self.backend_url}/api/models/basic",
            headers=headers,
            timeout=10
        )
        
        if response.status_code != 200:
            raise Exception(f"Model access failed: {response.status_code}")
    
    async def _test_premium_customer_flow(self) -> None:
        """Test premium customer features"""
        # Create premium test account
        response = requests.post(
            f"{self.backend_url}/api/test-accounts/create",
            json={'accountType': 'premium'},
            timeout=10
        )
        
        if response.status_code not in [200, 201]:
            raise Exception(f"Premium test account creation failed: {response.status_code}")
        
        account_data = response.json()
        headers = {'Authorization': f'Bearer {account_data.get("token")}'}
        
        # Test advanced features access
        advanced_endpoints = [
            '/api/models/advanced',
            '/api/backtesting/advanced',
            '/api/ai-insights',
            '/api/trading/paper-trading'
        ]
        
        for endpoint in advanced_endpoints:
            response = requests.get(
                f"{self.backend_url}{endpoint}",
                headers=headers,
                timeout=10
            )
            
            if response.status_code not in [200, 404]:  # 404 acceptable for some endpoints
                raise Exception(f"Premium feature access failed for {endpoint}: {response.status_code}")
    
    async def _test_institutional_flow(self) -> None:
        """Test institutional client features"""
        # Create institutional test account
        response = requests.post(
            f"{self.backend_url}/api/test-accounts/create",
            json={'accountType': 'institutional'},
            timeout=10
        )
        
        if response.status_code not in [200, 201]:
            raise Exception(f"Institutional test account creation failed: {response.status_code}")
        
        account_data = response.json()
        headers = {'Authorization': f'Bearer {account_data.get("token")}'}
        
        # Test large portfolio upload
        test_portfolio_path = project_root / 'backend' / 'test_data' / 'test_portfolio_10k_holdings.csv'
        
        with open(test_portfolio_path, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.backend_url}/api/portfolio/upload",
                files=files,
                headers=headers,
                timeout=60  # Longer timeout for large files
            )
        
        if response.status_code not in [200, 201]:
            raise Exception(f"Large portfolio upload failed: {response.status_code}")
        
        # Test enterprise features
        enterprise_endpoints = [
            '/api/models/enterprise',
            '/api/analytics/advanced',
            '/api/compliance/reporting'
        ]
        
        for endpoint in enterprise_endpoints:
            response = requests.get(
                f"{self.backend_url}{endpoint}",
                headers=headers,
                timeout=10
            )
            
            if response.status_code not in [200, 404]:
                raise Exception(f"Enterprise feature access failed for {endpoint}: {response.status_code}")
    
    async def _run_staff_workflow_tests(self) -> TestSuiteResult:
        """Test all staff workflows implemented by Agent 3"""
        self.logger.info("ðŸ‘¥ Running Staff Workflow Tests...")
        
        start_time = time.time()
        tests = []
        
        staff_roles = [
            'kyc_staff_dashboard',
            'support_staff_dashboard', 
            'trading_agent_dashboard',
            'enterprise_admin_dashboard'
        ]
        
        for role in staff_roles:
            try:
                result = await self._test_staff_workflow(role)
                tests.append(result)
            except Exception as e:
                tests.append(TestResult(
                    test_name=f"staff_workflow_{role}",
                    status='failed',
                    duration=0,
                    error_message=str(e)
                ))
        
        duration = time.time() - start_time
        passed = len([t for t in tests if t.status == 'passed'])
        failed = len([t for t in tests if t.status == 'failed'])
        skipped = len([t for t in tests if t.status == 'skipped'])
        
        return TestSuiteResult(
            suite_name='staff_workflows',
            total_tests=len(tests),
            passed=passed,
            failed=failed,
            skipped=skipped,
            duration=duration,
            tests=tests
        )
    
    async def _test_staff_workflow(self, role: str) -> TestResult:
        """Test specific staff workflow"""
        start_time = time.time()
        
        try:
            # Create staff test account
            response = requests.post(
                f"{self.backend_url}/api/test-accounts/create",
                json={'accountType': 'staff', 'role': role},
                timeout=10
            )
            
            if response.status_code not in [200, 201]:
                raise Exception(f"Staff account creation failed: {response.status_code}")
            
            account_data = response.json()
            headers = {'Authorization': f'Bearer {account_data.get("token")}'}
            
            # Test role-specific endpoints
            if role == 'kyc_staff_dashboard':
                await self._test_kyc_staff_features(headers)
            elif role == 'support_staff_dashboard':
                await self._test_support_staff_features(headers)
            elif role == 'trading_agent_dashboard':
                await self._test_trading_agent_features(headers)
            elif role == 'enterprise_admin_dashboard':
                await self._test_admin_features(headers)
            
            duration = time.time() - start_time
            return TestResult(
                test_name=f"staff_workflow_{role}",
                status='passed',
                duration=duration,
                details={'role': role}
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return TestResult(
                test_name=f"staff_workflow_{role}",
                status='failed',
                duration=duration,
                error_message=str(e)
            )
    
    async def _test_kyc_staff_features(self, headers: Dict[str, str]) -> None:
        """Test KYC staff specific features"""
        kyc_endpoints = [
            '/api/staff/kyc/pending-verifications',
            '/api/staff/kyc/document-reviews',
            '/api/staff/kyc/compliance-reports'
        ]
        
        for endpoint in kyc_endpoints:
            response = requests.get(
                f"{self.backend_url}{endpoint}",
                headers=headers,
                timeout=10
            )
            
            if response.status_code not in [200, 404]:
                raise Exception(f"KYC staff endpoint failed: {endpoint}")
    
    async def _test_support_staff_features(self, headers: Dict[str, str]) -> None:
        """Test support staff specific features"""
        support_endpoints = [
            '/api/staff/support/tickets',
            '/api/staff/support/user-accounts',
            '/api/staff/support/system-status'
        ]
        
        for endpoint in support_endpoints:
            response = requests.get(
                f"{self.backend_url}{endpoint}",
                headers=headers,
                timeout=10
            )
            
            if response.status_code not in [200, 404]:
                raise Exception(f"Support staff endpoint failed: {endpoint}")
    
    async def _test_trading_agent_features(self, headers: Dict[str, str]) -> None:
        """Test trading agent specific features"""
        trading_endpoints = [
            '/api/staff/trading/agent-performance',
            '/api/staff/trading/model-monitoring', 
            '/api/staff/trading/risk-management'
        ]
        
        for endpoint in trading_endpoints:
            response = requests.get(
                f"{self.backend_url}{endpoint}",
                headers=headers,
                timeout=10
            )
            
            if response.status_code not in [200, 404]:
                raise Exception(f"Trading agent endpoint failed: {endpoint}")
    
    async def _test_admin_features(self, headers: Dict[str, str]) -> None:
        """Test admin specific features"""
        admin_endpoints = [
            '/api/admin/users',
            '/api/admin/system-metrics',
            '/api/admin/audit-logs'
        ]
        
        for endpoint in admin_endpoints:
            response = requests.get(
                f"{self.backend_url}{endpoint}",
                headers=headers,
                timeout=10
            )
            
            if response.status_code not in [200, 404]:
                raise Exception(f"Admin endpoint failed: {endpoint}")
    
    async def _run_data_integration_tests(self) -> TestSuiteResult:
        """Test data validation and CSV import functionality (Agent 4)"""
        self.logger.info("ðŸ“Š Running Data Integration Tests...")
        
        start_time = time.time()
        tests = []
        
        # Test various data scenarios
        data_tests = [
            'portfolio_csv_validation',
            'large_portfolio_import',
            'malformed_data_handling',
            'edge_case_data_processing',
            'market_data_validation'
        ]
        
        for test_name in data_tests:
            try:
                result = await self._test_data_integration(test_name)
                tests.append(result)
            except Exception as e:
                tests.append(TestResult(
                    test_name=f"data_integration_{test_name}",
                    status='failed',
                    duration=0,
                    error_message=str(e)
                ))
        
        duration = time.time() - start_time
        passed = len([t for t in tests if t.status == 'passed'])
        failed = len([t for t in tests if t.status == 'failed'])
        skipped = len([t for t in tests if t.status == 'skipped'])
        
        return TestSuiteResult(
            suite_name='data_integration',
            total_tests=len(tests),
            passed=passed,
            failed=failed,
            skipped=skipped,
            duration=duration,
            tests=tests
        )
    
    async def _test_data_integration(self, test_name: str) -> TestResult:
        """Test specific data integration scenario"""
        start_time = time.time()
        
        try:
            # Create test account
            response = requests.post(
                f"{self.backend_url}/api/test-accounts/create",
                json={'accountType': 'verified'},
                timeout=10
            )
            
            if response.status_code not in [200, 201]:
                raise Exception(f"Test account creation failed: {response.status_code}")
            
            account_data = response.json()
            headers = {'Authorization': f'Bearer {account_data.get("token")}'}
            
            # Execute specific test
            if test_name == 'portfolio_csv_validation':
                await self._test_portfolio_validation(headers)
            elif test_name == 'large_portfolio_import':
                await self._test_large_portfolio_import(headers)
            elif test_name == 'malformed_data_handling':
                await self._test_malformed_data_handling(headers)
            elif test_name == 'edge_case_data_processing':
                await self._test_edge_case_processing(headers)
            elif test_name == 'market_data_validation':
                await self._test_market_data_validation(headers)
            
            duration = time.time() - start_time
            return TestResult(
                test_name=f"data_integration_{test_name}",
                status='passed',
                duration=duration,
                details={'test_type': test_name}
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return TestResult(
                test_name=f"data_integration_{test_name}",
                status='failed',
                duration=duration,
                error_message=str(e)
            )
    
    async def _test_portfolio_validation(self, headers: Dict[str, str]) -> None:
        """Test portfolio CSV validation"""
        test_file = project_root / 'backend' / 'test_data' / 'test_portfolio_small.csv'
        
        with open(test_file, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.backend_url}/api/portfolio/validate",
                files=files,
                headers=headers,
                timeout=30
            )
        
        if response.status_code not in [200, 201]:
            raise Exception(f"Portfolio validation failed: {response.status_code}")
        
        result = response.json()
        if not result.get('valid', False):
            raise Exception(f"Portfolio validation returned invalid: {result}")
    
    async def _test_large_portfolio_import(self, headers: Dict[str, str]) -> None:
        """Test large portfolio import performance"""
        test_file = project_root / 'backend' / 'test_data' / 'test_portfolio_10k_holdings.csv'
        
        start_time = time.time()
        with open(test_file, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.backend_url}/api/portfolio/upload",
                files=files,
                headers=headers,
                timeout=120  # Extended timeout for large files
            )
        
        duration = time.time() - start_time
        
        if response.status_code not in [200, 201]:
            raise Exception(f"Large portfolio import failed: {response.status_code}")
        
        # Check performance - should complete within reasonable time
        if duration > 60:  # 60 seconds threshold
            raise Exception(f"Large portfolio import too slow: {duration:.2f}s")
    
    async def _test_malformed_data_handling(self, headers: Dict[str, str]) -> None:
        """Test handling of malformed data"""
        test_file = project_root / 'backend' / 'test_data' / 'test_portfolio_malformed.csv'
        
        with open(test_file, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.backend_url}/api/portfolio/validate",
                files=files,
                headers=headers,
                timeout=30
            )
        
        # Should return validation errors, not crash
        if response.status_code == 500:
            raise Exception("Server crashed on malformed data")
        
        result = response.json()
        if result.get('valid', True):  # Should be invalid
            raise Exception("Malformed data was incorrectly validated as valid")
    
    async def _test_edge_case_processing(self, headers: Dict[str, str]) -> None:
        """Test edge case data processing"""
        test_file = project_root / 'backend' / 'test_data' / 'test_portfolio_edge_cases.csv'
        
        with open(test_file, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.backend_url}/api/portfolio/upload",
                files=files,
                headers=headers,
                timeout=30
            )
        
        if response.status_code not in [200, 201, 400]:  # 400 acceptable for edge cases
            raise Exception(f"Edge case processing failed: {response.status_code}")
    
    async def _test_market_data_validation(self, headers: Dict[str, str]) -> None:
        """Test market data validation"""
        response = requests.get(
            f"{self.backend_url}/api/market/validate",
            headers=headers,
            timeout=10
        )
        
        if response.status_code not in [200, 404]:
            raise Exception(f"Market data validation failed: {response.status_code}")
    
    async def _run_system_integration_tests(self) -> TestSuiteResult:
        """Test system integration across all components"""
        self.logger.info("ðŸ”— Running System Integration Tests...")
        
        start_time = time.time()
        tests = []
        
        integration_tests = [
            'api_gateway_integration',
            'database_connectivity',
            'websocket_functionality', 
            'microservice_communication',
            'caching_system_integration'
        ]
        
        for test_name in integration_tests:
            try:
                result = await self._test_system_integration(test_name)
                tests.append(result)
            except Exception as e:
                tests.append(TestResult(
                    test_name=f"system_integration_{test_name}",
                    status='failed',
                    duration=0,
                    error_message=str(e)
                ))
        
        duration = time.time() - start_time
        passed = len([t for t in tests if t.status == 'passed'])
        failed = len([t for t in tests if t.status == 'failed'])
        skipped = len([t for t in tests if t.status == 'skipped'])
        
        return TestSuiteResult(
            suite_name='system_integration',
            total_tests=len(tests),
            passed=passed,
            failed=failed,
            skipped=skipped,
            duration=duration,
            tests=tests
        )
    
    async def _test_system_integration(self, test_name: str) -> TestResult:
        """Test specific system integration"""
        start_time = time.time()
        
        try:
            if test_name == 'api_gateway_integration':
                await self._test_api_gateway()
            elif test_name == 'database_connectivity':
                await self._test_database_connectivity()
            elif test_name == 'websocket_functionality':
                await self._test_websocket_functionality()
            elif test_name == 'microservice_communication':
                await self._test_microservice_communication()
            elif test_name == 'caching_system_integration':
                await self._test_caching_system()
            
            duration = time.time() - start_time
            return TestResult(
                test_name=f"system_integration_{test_name}",
                status='passed',
                duration=duration
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return TestResult(
                test_name=f"system_integration_{test_name}",
                status='failed',
                duration=duration,
                error_message=str(e)
            )
    
    async def _test_api_gateway(self) -> None:
        """Test API gateway functionality"""
        response = requests.get(f"{self.backend_url}/health", timeout=10)
        if response.status_code != 200:
            raise Exception(f"API gateway health check failed: {response.status_code}")
    
    async def _test_database_connectivity(self) -> None:
        """Test database connectivity"""
        response = requests.get(f"{self.backend_url}/api/test/db-health", timeout=10)
        if response.status_code not in [200, 404]:
            raise Exception(f"Database connectivity test failed: {response.status_code}")
    
    async def _test_websocket_functionality(self) -> None:
        """Test WebSocket functionality"""
        # Basic WebSocket connection test
        try:
            import websocket
            ws_url = self.backend_url.replace('http', 'ws') + '/ws/test'
            ws = websocket.create_connection(ws_url, timeout=10)
            ws.send("test_message")
            result = ws.recv()
            ws.close()
            
            if not result:
                raise Exception("WebSocket test failed - no response")
                
        except Exception as e:
            # WebSocket might not be implemented yet, so we'll make this a warning
            self.logger.warning(f"WebSocket test failed (may not be implemented): {e}")
    
    async def _test_microservice_communication(self) -> None:
        """Test communication between microservices"""
        # Test various microservice endpoints
        services = [
            '/api/models/health',
            '/api/portfolio/health', 
            '/api/market/health'
        ]
        
        for service in services:
            response = requests.get(f"{self.backend_url}{service}", timeout=10)
            if response.status_code not in [200, 404]:
                self.logger.warning(f"Microservice {service} health check inconclusive: {response.status_code}")
    
    async def _test_caching_system(self) -> None:
        """Test caching system integration"""
        # Test cache endpoints if available
        response = requests.get(f"{self.backend_url}/api/cache/health", timeout=10)
        if response.status_code not in [200, 404]:
            self.logger.warning(f"Cache system test inconclusive: {response.status_code}")
    
    async def _run_performance_tests(self) -> TestSuiteResult:
        """Run performance and load tests"""
        self.logger.info("âš¡ Running Performance Tests...")
        
        start_time = time.time()
        tests = []
        
        performance_tests = [
            'api_response_time',
            'concurrent_user_load',
            'memory_usage_monitoring',
            'database_query_performance',
            'file_upload_performance'
        ]
        
        for test_name in performance_tests:
            try:
                result = await self._test_performance(test_name)
                tests.append(result)
            except Exception as e:
                tests.append(TestResult(
                    test_name=f"performance_{test_name}",
                    status='failed',
                    duration=0,
                    error_message=str(e)
                ))
        
        duration = time.time() - start_time
        passed = len([t for t in tests if t.status == 'passed'])
        failed = len([t for t in tests if t.status == 'failed'])
        skipped = len([t for t in tests if t.status == 'skipped'])
        
        return TestSuiteResult(
            suite_name='performance',
            total_tests=len(tests),
            passed=passed,
            failed=failed,
            skipped=skipped,
            duration=duration,
            tests=tests
        )
    
    async def _test_performance(self, test_name: str) -> TestResult:
        """Test specific performance metric"""
        start_time = time.time()
        
        try:
            if test_name == 'api_response_time':
                await self._test_api_response_time()
            elif test_name == 'concurrent_user_load':
                await self._test_concurrent_load()
            elif test_name == 'memory_usage_monitoring':
                await self._test_memory_usage()
            elif test_name == 'database_query_performance':
                await self._test_db_performance()
            elif test_name == 'file_upload_performance':
                await self._test_upload_performance()
            
            duration = time.time() - start_time
            return TestResult(
                test_name=f"performance_{test_name}",
                status='passed',
                duration=duration
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return TestResult(
                test_name=f"performance_{test_name}",
                status='failed',
                duration=duration,
                error_message=str(e)
            )
    
    async def _test_api_response_time(self) -> None:
        """Test API response time performance"""
        baseline = self.config['performance_baseline']['api_response_time']
        
        endpoints = [
            '/health',
            '/api/user/dashboard',
            '/api/models/basic',
            '/api/market/status'
        ]
        
        for endpoint in endpoints:
            start_time = time.time()
            response = requests.get(f"{self.backend_url}{endpoint}", timeout=10)
            duration = time.time() - start_time
            
            if duration > baseline:
                raise Exception(f"API response time too slow for {endpoint}: {duration:.2f}s > {baseline}s")
            
            if response.status_code not in [200, 404]:
                self.logger.warning(f"API endpoint {endpoint} returned {response.status_code}")
    
    async def _test_concurrent_load(self) -> None:
        """Test concurrent user load"""
        import concurrent.futures
        
        def make_request():
            response = requests.get(f"{self.backend_url}/health", timeout=10)
            return response.status_code == 200
        
        # Test with 10 concurrent requests
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(make_request) for _ in range(10)]
            results = [f.result() for f in futures]
        
        success_rate = sum(results) / len(results)
        if success_rate < 0.95:  # 95% success rate threshold
            raise Exception(f"Concurrent load test failed: {success_rate:.2%} success rate")
    
    async def _test_memory_usage(self) -> None:
        """Test memory usage monitoring"""
        baseline = self.config['performance_baseline']['memory_usage']
        
        # Get current process memory usage
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > baseline:
            self.logger.warning(f"Memory usage above baseline: {memory_mb:.2f}MB > {baseline}MB")
    
    async def _test_db_performance(self) -> None:
        """Test database query performance"""
        # Test simple database query performance
        start_time = time.time()
        response = requests.get(f"{self.backend_url}/api/test/db-query", timeout=10)
        duration = time.time() - start_time
        
        if response.status_code == 200 and duration > 5.0:  # 5 second threshold
            raise Exception(f"Database query too slow: {duration:.2f}s")
    
    async def _test_upload_performance(self) -> None:
        """Test file upload performance"""
        test_file = project_root / 'backend' / 'test_data' / 'test_portfolio_small.csv'
        
        # Create test account
        response = requests.post(
            f"{self.backend_url}/api/test-accounts/create",
            json={'accountType': 'verified'},
            timeout=10
        )
        
        if response.status_code in [200, 201]:
            account_data = response.json()
            headers = {'Authorization': f'Bearer {account_data.get("token")}'}
            
            start_time = time.time()
            with open(test_file, 'rb') as f:
                files = {'file': f}
                response = requests.post(
                    f"{self.backend_url}/api/portfolio/upload",
                    files=files,
                    headers=headers,
                    timeout=30
                )
            duration = time.time() - start_time
            
            if response.status_code in [200, 201] and duration > 10.0:  # 10 second threshold
                raise Exception(f"File upload too slow: {duration:.2f}s")
    
    async def _run_security_tests(self) -> TestSuiteResult:
        """Run security and vulnerability tests"""
        self.logger.info("ðŸ”’ Running Security Tests...")
        
        start_time = time.time()
        tests = []
        
        security_tests = [
            'authentication_security',
            'authorization_checks',
            'input_validation_security',
            'file_upload_security',
            'sql_injection_protection',
            'xss_protection'
        ]
        
        for test_name in security_tests:
            try:
                result = await self._test_security(test_name)
                tests.append(result)
            except Exception as e:
                tests.append(TestResult(
                    test_name=f"security_{test_name}",
                    status='failed',
                    duration=0,
                    error_message=str(e)
                ))
        
        duration = time.time() - start_time
        passed = len([t for t in tests if t.status == 'passed'])
        failed = len([t for t in tests if t.status == 'failed'])
        skipped = len([t for t in tests if t.status == 'skipped'])
        
        return TestSuiteResult(
            suite_name='security',
            total_tests=len(tests),
            passed=passed,
            failed=failed,
            skipped=skipped,
            duration=duration,
            tests=tests
        )
    
    async def _test_security(self, test_name: str) -> TestResult:
        """Test specific security aspect"""
        start_time = time.time()
        
        try:
            if test_name == 'authentication_security':
                await self._test_authentication_security()
            elif test_name == 'authorization_checks':
                await self._test_authorization_checks()
            elif test_name == 'input_validation_security':
                await self._test_input_validation()
            elif test_name == 'file_upload_security':
                await self._test_file_upload_security()
            elif test_name == 'sql_injection_protection':
                await self._test_sql_injection_protection()
            elif test_name == 'xss_protection':
                await self._test_xss_protection()
            
            duration = time.time() - start_time
            return TestResult(
                test_name=f"security_{test_name}",
                status='passed',
                duration=duration
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return TestResult(
                test_name=f"security_{test_name}",
                status='failed',
                duration=duration,
                error_message=str(e)
            )
    
    async def _test_authentication_security(self) -> None:
        """Test authentication security"""
        # Test invalid credentials
        response = requests.post(
            f"{self.backend_url}/api/auth/login",
            json={'email': 'invalid@example.com', 'password': 'wrongpassword'},
            timeout=10
        )
        
        if response.status_code == 200:
            raise Exception("Authentication allowed invalid credentials")
        
        # Test without token
        response = requests.get(f"{self.backend_url}/api/user/dashboard", timeout=10)
        if response.status_code == 200:
            raise Exception("Protected endpoint accessible without authentication")
    
    async def _test_authorization_checks(self) -> None:
        """Test authorization and role-based access"""
        # Create basic user
        response = requests.post(
            f"{self.backend_url}/api/test-accounts/create",
            json={'accountType': 'basic'},
            timeout=10
        )
        
        if response.status_code in [200, 201]:
            account_data = response.json()
            headers = {'Authorization': f'Bearer {account_data.get("token")}'}
            
            # Try to access admin endpoint
            response = requests.get(
                f"{self.backend_url}/api/admin/users",
                headers=headers,
                timeout=10
            )
            
            if response.status_code == 200:
                raise Exception("Basic user accessed admin endpoint")
    
    async def _test_input_validation(self) -> None:
        """Test input validation security"""
        # Test malicious input
        malicious_inputs = [
            "'; DROP TABLE users; --",
            "<script>alert('xss')</script>",
            "../../../../etc/passwd",
            "javascript:alert('xss')"
        ]
        
        for malicious_input in malicious_inputs:
            response = requests.post(
                f"{self.backend_url}/api/auth/register",
                json={
                    'email': malicious_input,
                    'password': 'password',
                    'firstName': malicious_input,
                    'lastName': malicious_input
                },
                timeout=10
            )
            
            # Should reject malicious input
            if response.status_code == 200:
                self.logger.warning(f"Potentially unsafe input accepted: {malicious_input}")
    
    async def _test_file_upload_security(self) -> None:
        """Test file upload security"""
        # Create test account
        response = requests.post(
            f"{self.backend_url}/api/test-accounts/create",
            json={'accountType': 'verified'},
            timeout=10
        )
        
        if response.status_code in [200, 201]:
            account_data = response.json()
            headers = {'Authorization': f'Bearer {account_data.get("token")}'}
            
            # Test malicious file upload
            malicious_content = b"<?php echo 'malicious code'; ?>"
            files = {'file': ('malicious.php', malicious_content, 'application/php')}
            
            response = requests.post(
                f"{self.backend_url}/api/portfolio/upload",
                files=files,
                headers=headers,
                timeout=10
            )
            
            # Should reject non-CSV files
            if response.status_code == 200:
                raise Exception("Malicious file upload was accepted")
    
    async def _test_sql_injection_protection(self) -> None:
        """Test SQL injection protection"""
        sql_injection_attempts = [
            "1' OR '1'='1",
            "1; DROP TABLE users; --",
            "' UNION SELECT * FROM users --"
        ]
        
        for injection in sql_injection_attempts:
            response = requests.get(
                f"{self.backend_url}/api/user/profile",
                params={'id': injection},
                timeout=10
            )
            
            # Should not return sensitive data or crash
            if response.status_code == 500:
                self.logger.warning(f"Potential SQL injection vulnerability: {injection}")
    
    async def _test_xss_protection(self) -> None:
        """Test XSS protection"""
        xss_payloads = [
            "<script>alert('xss')</script>",
            "javascript:alert('xss')",
            "<img src=x onerror=alert('xss')>"
        ]
        
        for payload in xss_payloads:
            response = requests.post(
                f"{self.backend_url}/api/user/profile",
                json={'name': payload, 'bio': payload},
                timeout=10
            )
            
            # Should sanitize input
            if response.status_code == 200:
                result = response.json()
                if payload in str(result):
                    self.logger.warning(f"Potential XSS vulnerability: {payload}")
    
    async def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report"""
        self.logger.info("ðŸ“‹ Generating comprehensive test report...")
        
        total_duration = datetime.datetime.now() - self.start_time
        
        # Calculate overall statistics
        total_tests = sum(suite.total_tests for suite in self.test_results.values())
        total_passed = sum(suite.passed for suite in self.test_results.values())
        total_failed = sum(suite.failed for suite in self.test_results.values())
        total_skipped = sum(suite.skipped for suite in self.test_results.values())
        
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        # Identify critical failures
        critical_failures = []
        for suite_name, suite in self.test_results.items():
            for test in suite.tests:
                if test.status == 'failed':
                    critical_failures.append({
                        'suite': suite_name,
                        'test': test.test_name,
                        'error': test.error_message
                    })
        
        report = {
            'timestamp': datetime.datetime.now().isoformat(),
            'duration': str(total_duration),
            'summary': {
                'total_tests': total_tests,
                'passed': total_passed,
                'failed': total_failed,
                'skipped': total_skipped,
                'success_rate': f"{success_rate:.2f}%"
            },
            'test_suites': {},
            'critical_failures': critical_failures,
            'environment': {
                'backend_url': self.backend_url,
                'frontend_url': self.frontend_url,
                'test_framework_version': '1.0.0'
            },
            'recommendations': self._generate_recommendations()
        }
        
        # Add detailed suite results
        for suite_name, suite in self.test_results.items():
            report['test_suites'][suite_name] = {
                'total_tests': suite.total_tests,
                'passed': suite.passed,
                'failed': suite.failed,
                'skipped': suite.skipped,
                'duration': f"{suite.duration:.2f}s",
                'success_rate': f"{(suite.passed / suite.total_tests * 100):.2f}%" if suite.total_tests > 0 else "0%",
                'tests': [
                    {
                        'name': test.test_name,
                        'status': test.status,
                        'duration': f"{test.duration:.2f}s",
                        'error': test.error_message
                    }
                    for test in suite.tests
                ]
            }
        
        # Save report to file
        report_dir = project_root / 'logs' / 'testing' / 'reports'
        report_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = report_dir / f'continuous_testing_report_{timestamp}.json'
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"ðŸ“Š Test report saved: {report_file}")
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []
        
        # Check success rates
        for suite_name, suite in self.test_results.items():
            success_rate = (suite.passed / suite.total_tests * 100) if suite.total_tests > 0 else 0
            
            if success_rate < 80:
                recommendations.append(f"âŒ {suite_name} has low success rate ({success_rate:.1f}%) - requires immediate attention")
            elif success_rate < 95:
                recommendations.append(f"âš ï¸ {suite_name} has moderate success rate ({success_rate:.1f}%) - consider investigation")
        
        # Check performance
        if 'performance' in self.test_results:
            perf_suite = self.test_results['performance']
            if perf_suite.failed > 0:
                recommendations.append("âš¡ Performance tests failing - check system resources and optimization")
        
        # Check security
        if 'security' in self.test_results:
            sec_suite = self.test_results['security']
            if sec_suite.failed > 0:
                recommendations.append("ðŸ”’ Security tests failing - immediate security review required")
        
        # General recommendations
        if not recommendations:
            recommendations.append("âœ… All test suites performing well - system ready for production")
        
        return recommendations
    
    async def _send_notifications(self, report: Dict[str, Any]) -> None:
        """Send test results notifications"""
        if not self.config.get('notification', {}).get('email_enabled', False):
            return
        
        self.logger.info("ðŸ“§ Sending test result notifications...")
        
        # Format notification message
        summary = report['summary']
        success_rate = float(summary['success_rate'].replace('%', ''))
        
        status_emoji = "âœ…" if success_rate >= 95 else "âš ï¸" if success_rate >= 80 else "âŒ"
        
        message = f"""
{status_emoji} Qlib Trading Platform - Daily Test Results

ðŸ“Š Test Summary:
- Total Tests: {summary['total_tests']}
- Passed: {summary['passed']}
- Failed: {summary['failed']}
- Success Rate: {summary['success_rate']}
- Duration: {report['duration']}

ðŸ” Test Suites:
"""
        
        for suite_name, suite_data in report['test_suites'].items():
            message += f"- {suite_name}: {suite_data['success_rate']} ({suite_data['passed']}/{suite_data['total_tests']})\n"
        
        if report['critical_failures']:
            message += f"\nâŒ Critical Failures ({len(report['critical_failures'])}):\n"
            for failure in report['critical_failures'][:5]:  # Show first 5
                message += f"- {failure['suite']}: {failure['test']}\n"
        
        message += f"\nðŸ’¡ Recommendations:\n"
        for rec in report['recommendations'][:3]:  # Show first 3
            message += f"- {rec}\n"
        
        # Send via configured notification channels
        await self._send_slack_notification(message, report)
        await self._send_email_notification(message, report)
    
    async def _send_slack_notification(self, message: str, report: Dict[str, Any]) -> None:
        """Send Slack notification"""
        webhook_url = self.config.get('notification', {}).get('slack_webhook')
        if not webhook_url:
            return
        
        try:
            payload = {
                'text': f"Qlib Trading Platform - Test Results",
                'attachments': [{
                    'color': 'good' if float(report['summary']['success_rate'].replace('%', '')) >= 95 else 'warning',
                    'text': message,
                    'footer': 'Qlib Continuous Testing Framework',
                    'ts': int(time.time())
                }]
            }
            
            response = requests.post(webhook_url, json=payload, timeout=10)
            if response.status_code == 200:
                self.logger.info("âœ… Slack notification sent successfully")
            else:
                self.logger.error(f"âŒ Slack notification failed: {response.status_code}")
                
        except Exception as e:
            self.logger.error(f"âŒ Slack notification error: {e}")
    
    async def _send_email_notification(self, message: str, report: Dict[str, Any]) -> None:
        """Send email notification"""
        # Email notification implementation would go here
        # For now, just log the attempt
        self.logger.info("ðŸ“§ Email notification prepared (implementation required)")

# CLI Interface
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Qlib Continuous Testing Framework')
    parser.add_argument('--config', help='Configuration file path')
    parser.add_argument('--backend-url', default='http://localhost:8000', help='Backend URL')
    parser.add_argument('--frontend-url', default='http://localhost:5173', help='Frontend URL')
    parser.add_argument('--verbose', action='store_true', help='Verbose logging')
    
    args = parser.parse_args()
    
    # Override config with CLI args
    config_overrides = {}
    if args.backend_url:
        config_overrides['backend_url'] = args.backend_url
    if args.frontend_url:
        config_overrides['frontend_url'] = args.frontend_url
    
    # Initialize framework
    framework = ContinuousTestingFramework(args.config)
    if config_overrides:
        framework.config.update(config_overrides)
    
    if args.verbose:
        framework.logger.setLevel(logging.DEBUG)
    
    # Run tests
    async def main():
        try:
            results = await framework.run_daily_test_suite()
            
            # Print summary
            total_tests = sum(suite.total_tests for suite in results.values())
            total_passed = sum(suite.passed for suite in results.values())
            success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
            
            print(f"\nðŸŽ¯ TESTING SUMMARY")
            print(f"Total Tests: {total_tests}")
            print(f"Passed: {total_passed}")
            print(f"Success Rate: {success_rate:.2f}%")
            
            if success_rate >= 95:
                print("âœ… System ready for production")
                exit(0)
            elif success_rate >= 80:
                print("âš ï¸ System has issues - review required")
                exit(1)
            else:
                print("âŒ System has critical issues - deployment blocked")
                exit(2)
                
        except Exception as e:
            print(f"âŒ Testing framework failed: {e}")
            exit(3)
    
    asyncio.run(main())